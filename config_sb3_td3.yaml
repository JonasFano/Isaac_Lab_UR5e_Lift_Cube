# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/sac.yml
program: train_sb3_wandb_td3.py
method: bayes
name: rel_ik_sb3_td3_ur5e_lift_cube_0_05_bayes
metric:
  goal: maximize
  name: rollout/ep_rew_mean

parameters:
  seed:
    value: 42

  device:
    value: "cuda:0"

  n_timesteps: # iteration * n_steps * nenvs: 400 * 64 * 8192 = 209715200
    value: 209715200

  policy:
    value: 'MlpPolicy'

  batch_size:
    values: [64, 128, 256, 512]

  gamma:
    min: 0.9
    max: 0.999

  learning_rate:
    min: !!float 1e-5
    max: !!float 1e-3

  train_freq:
    values: [1, 4, 8, 16]

  gradient_steps:
    values: [1, 4, 8, 16]

  buffer_size: 
    values: [50000, 100000, 1000000, 5000000]

  policy_delay:
    values: [1, 2, 3]

  learning_starts: 
    values: [100, 1000, 5000, 10000]

  tau: 
    min: 0.005
    max: 0.05

  target_policy_noise:
    min: 0.1
    max: 0.3

  target_noise_clip:
    min: 0.2
    max: 0.6

  action_noise:
    value: NormalActionNoise

  action_sigma:
    min: 0.001
    max: 0.1

  # replay_buffer_class: 
  #   value: "HerReplayBuffer"

  # replay_buffer_kwargs: 
  #   value: "dict(goal_selection_strategy='future', n_sampled_goal=4)"

  policy_kwargs:
    parameters:
      activation_fn: 
        values: ['nn.ELU', 'nn.ReLU', 'nn.Tanh']
      net_arch:
        values: [[64, 64], [128, 128], [256, 256], [400, 300]]


  normalize_input:
    value: False

  normalize_value:
    value: False

  clip_obs:
    value: 50.0


#######################
# Previous parameters #
#######################


# program: train_sb3_wandb_td3.py
# method: grid
# name: rel_ik_sb3_td3_ur5e_lift_cube_0_05_noise_100
# metric:
#   goal: maximize
#   name: rollout/ep_rew_mean

# parameters:
#   seed:
#     value: 42

#   device:
#     value: "cuda:0"

#   n_timesteps: # iteration * n_steps * nenvs: 400 * 64 * 8192 = 209715200
#     value: 209715200

#   policy:
#     value: 'MlpPolicy'

#   batch_size:
#     value: 256 

#   gamma:
#     value: 0.95

#   learning_rate:
#     value: 3e-4

#   train_freq:
#     value: 4

#   gradient_steps:
#     value: 4

#   buffer_size: 
#     value: 1000000

#   policy_delay:
#     value: 2

#   learning_starts: 
#     value: 1000

#   tau: 
#     value: 0.02

#   target_policy_noise:
#     value: 0.2

#   target_noise_clip:
#     value: 0.5

#   action_noise:
#     value: NormalActionNoise

#   # replay_buffer_class: 
#   #   value: "HerReplayBuffer"

#   # replay_buffer_kwargs: 
#   #   value: "dict(goal_selection_strategy='future', n_sampled_goal=4)"

#   policy_kwargs:
#     parameters:
#       activation_fn: 
#         values: ['nn.ELU', 'nn.ReLU', 'nn.Tanh']
#       net_arch:
#         value: [256, 256]


#   normalize_input:
#     value: False

#   normalize_value:
#     value: False

#   clip_obs:
#     value: 50.0